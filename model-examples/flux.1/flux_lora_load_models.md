# æ¨¡å‹åŠ è½½ã€LoRAå±‚åˆå§‹åŒ–ä¸æ¨¡å—ç²¾åº¦è®¾ç½®

æœ¬æ–‡ä»‹ç» Diffsuers-style FLUX.1 Dreambooth LoRA å¾®è°ƒå¼€å‘å®è·µä¸­çš„æ¨¡å‹å®šä¹‰ã€é¢„è®­ç»ƒæƒé‡åŠ è½½ã€LoRAå±‚åˆå§‹åŒ–ä¸æ¨¡å—ç²¾åº¦éƒ¨åˆ†ï¼Œåˆ†äº«å¼€å‘æ—¶å¯èƒ½éœ€è¦æ³¨æ„çš„è¦ç‚¹ã€‚


## æ¨¡å‹åŠ è½½

æ¨¡å‹çš„åŠ è½½å¯ç›´æ¥è°ƒç”¨ MindONE.diffusers ä¸­å·²ç»å®šä¹‰å¥½çš„æ¨¡å‹æ¥å£ï¼Œè°ƒç”¨è¿‡ç¨‹å‡ ä¹æ— éœ€å…³å¿ƒ huggingface diffusers ä¸ MindONE.diffusers æ¨¡å‹å®šä¹‰è¿‡ç¨‹ä¸­çš„åŒºåˆ«ã€‚å¦‚æœä½ ä¹‹å‰æ²¡æœ‰ç”¨è¿‡ diffusers å·¥å…·ï¼Œåˆ™å¯ä»¥ç»§ç»­çœ‹æœ¬èŠ‚æ˜¯å¦‚ä½•å®šä¹‰æ¨¡å‹çš„ã€‚

FLUX.1 å¾®è°ƒéœ€è¦åŠ è½½çš„æ¨¡å‹/æ¨¡å—æœ‰ï¼š
- æ–‡æœ¬ç¼–ç ï¼šCLIP & T5
- å›¾åƒç¼–è§£ç ï¼švae 
- å™ªå£°é¢„æµ‹ ï¼šflux_transformers
- schedulerï¼šFlowMatchEulerDiscreteScheduler


MindONE.diffusers æ”¯æŒç›´æ¥åŠ è½½ safetensoer æ ¼å¼çš„æ¨¡å‹ï¼Œå½“ç„¶ MindSpore æ¡†æ¶æœ¬èº«ä¹Ÿæ˜¯æ”¯æŒ safetensors æƒé‡çš„åŠ è½½ä¸ä¿å­˜çš„ï¼Œè¯¦æƒ…å¯ä»¥æŸ¥çœ‹æ¥å£[load_checkpoint](https://www.mindspore.cn/docs/zh-CN/master/api_python/mindspore/mindspore.load_checkpoint.html?highlight=load_checkpoint#mindspore.load_checkpoint), [save_checkpoint](https://www.mindspore.cn/docs/zh-CN/master/api_python/mindspore/mindspore.save_checkpoint.html?highlight=save_checkpoint#mindspore.save_checkpoint) çš„å…¥å‚ `format`ã€‚ 
 
è„šæœ¬çš„ä¼ å‚ `args.pretrained_model_name_or_path` ä¼ å‚å¯ä»¥ç›´æ¥ä¼  huggingface ç¤¾åŒºä¸Šçš„æ¨¡å‹çš„ model name, `from_pretrained` æ¥å£ä¼šè‡ªåŠ¨ä» hf ç¤¾åŒºä¸‹è½½é…ç½®æ–‡ä»¶ä¸æƒé‡åˆ°ç¼“å­˜è·¯å¾„ã€‚ æˆ–è€…æå‰ä¸‹è½½å¥½ä¸€å¥—æƒé‡ï¼ŒæŒ‰ç…§æŒ‡å®šæ ¼å¼æ‘†æ”¾ï¼Œä¼ å‚æ—¶ä¼ æœ¬åœ°æƒé‡è·¯å¾„ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev/tree/main)ã€‚

æ–‡æœ¬ç¼–ç éƒ¨åˆ†ï¼Œtokenizers ç›´æ¥ä» `transformers` åº“è°ƒç”¨ã€‚text encoders æ¨¡å‹éƒ¨åˆ†ï¼Œæˆ‘ä»¬åœ¨ `MindONE.transformers` ç»„ä»¶å¯¹å¤§éƒ¨åˆ† diffusers ä½¿ç”¨åˆ°çš„æ–‡æœ¬ç¼–ç æ¨¡å‹åšäº† mindspore é€‚é…ï¼Œæ­¤å¤„å› æ­¤ç”¨åˆ°çš„æ¨¡å‹ç›´æ¥ä» `MindONE.transformers` åŠ è½½ã€‚

```python
from transformers import CLIPTokenizer, PretrainedConfig, T5TokenizerFast
from mindone.transformers import CLIPTextModel, T5EncoderModel

def import_model_class_from_model_name_or_path(
    pretrained_model_name_or_path: str, revision: str, subfolder: str = "text_encoder"
):
    text_encoder_config = PretrainedConfig.from_pretrained(
        pretrained_model_name_or_path, subfolder=subfolder, revision=revision
    )
    model_class = text_encoder_config.architectures[0]
    if model_class == "CLIPTextModel":
        return CLIPTextModel
    
    elif model_class == "T5EncoderModel":
        return T5EncoderModel
    else:
        raise ValueError(f"{model_class} is not supported.")

def load_text_encoders(args, class_one, class_two):
    text_encoder_one = class_one.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="text_encoder", revision=args.revision, variant=args.variant
    )
    text_encoder_two = class_two.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="text_encoder_2", revision=args.revision, variant=args.variant
    )
    return text_encoder_one, text_encoder_two


# Load the tokenizers
tokenizer_one = CLIPTokenizer.from_pretrained(
    args.pretrained_model_name_or_path,
    subfolder="tokenizer",
    revision=args.revision,
)
tokenizer_two = T5TokenizerFast.from_pretrained(
    args.pretrained_model_name_or_path,
    subfolder="tokenizer_2",
    revision=args.revision,
)


# import correct text encoder classes
text_encoder_cls_one = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)
text_encoder_cls_two = import_model_class_from_model_name_or_path(
    args.pretrained_model_name_or_path, args.revision, subfolder="text_encoder_2"
)

# load text encoders
text_encoder_one, text_encoder_two = load_text_encoders(args, text_encoder_cls_one, text_encoder_cls_two)
```

vaeã€flux_transformersã€noise_scheduler ç›´æ¥ä» `MindONE.diffusers` è°ƒç”¨ï¼š


```python
from mindone.diffusers import AutoencoderKL, FlowMatchEulerDiscreteScheduler, FluxPipeline, FluxTransformer2DModel

# Load scheduler and models
noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(
    args.pretrained_model_name_or_path, subfolder="scheduler"
)

vae = AutoencoderKL.from_pretrained(
    args.pretrained_model_name_or_path,
    subfolder="vae",
    revision=args.revision,
    variant=args.variant,
)
transformer = FluxTransformer2DModel.from_pretrained(
    args.pretrained_model_name_or_path, subfolder="transformer", revision=args.revision, variant=args.variant
)
```

é€šå¸¸æ¨¡å‹åˆå§‹åŒ–åæ ¹æ®å„ä¸ªç»„ä»¶æ˜¯å¦éœ€è¦è®­ç»ƒæ¥è®¾ç½®å„ä¸ªç»„ä»¶ä¸­å‚æ•°çš„ `param.requires_grad` å€¼ï¼Œæˆ‘ä»¬åªå¾®è°ƒ transformer æ³¨å…¥çš„ lora å±‚ï¼Œæ‰€ä»¥å…ˆæŠŠä¸Šé¢å®šä¹‰å¥½çš„æ‰€æœ‰æ¨¡å‹ç»„ä»¶çš„å‚æ•°è®¾ç½® `requires_grad = False`ã€‚MindSpore Cell æš‚æ—¶æ²¡æœ‰`requires_grad_`æ¥å£ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸‹é¢æ–¹æ³•åœ¨è®­ç»ƒè„šæœ¬ä¸­ç­‰ä»·å®ç°å‚æ•°å†»ç»“ã€‚


```python
# We only train the additional adapter LoRA layers
from mindspore import nn
def freeze_params(m: nn.Cell):
    for p in m.get_parameters():
        p.requires_grad = False

freeze_params(transformer)
freeze_params(vae)
freeze_params(text_encoder_one)
freeze_params(text_encoder_two)
```

## LoRA å±‚åˆå§‹åŒ–

mindONE.diffusers é›†æˆäº† ğŸ¤—PEFT (Parameter-Efficient Fine-Tuning) åº“ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥æ–¹ä¾¿åœ°æ³¨å…¥å¾®è°ƒå±‚ã€‚mindONE.diffusers æ¯ä¸ªå…·ä½“æ¨¡å‹ç»§æ‰¿çš„æŠ½è±¡ç±»ä¹‹ä¸€æ˜¯ `PeftAdapterMixin` ï¼Œå®ƒåŒ…å«ç”¨äºåŠ è½½å’Œä½¿ç”¨ PEFT åº“ä¸­æ”¯æŒçš„ adapters weights çš„æ‰€æœ‰å‡½æ•°ï¼Œå…¶ä¸­å°±åŒ…æ‹¬ LoRA å±‚æ³¨å…¥æ–¹æ³•ã€‚ä¾‹å¦‚æˆ‘ä»¬è¦åš LoRA å¾®è°ƒçš„æ¨¡å‹ `FluxTransformer2DModel`ï¼Œå®šä¹‰ç¤ºä¾‹å¦‚ä¸‹ï¼š


```python
class FluxTransformer2DModel(ModelMixin, ConfigMixin, PeftAdapterMixin, FromOriginalModelMixin):
    ...
```

ä»¥ä¸‹æ˜¯æˆ‘ä»¬é€šè¿‡è°ƒç”¨ `PeftAdapterMixin` çš„ `add_adapter` æ–¹æ³•ä¸ºå¾®è°ƒæ¨¡å‹æ³¨å…¥ LoRA å±‚çš„æ ·ä¾‹ä»£ç ã€‚æˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡ `LoraConfig` çš„ `target_modules` æŒ‡å®šå…·ä½“å¾®è°ƒæ³¨å…¥çš„æ¨¡å—ã€æŒ‡å®š LoRA çš„ç§©ä»¥åŠ LoRA å±‚æƒé‡åˆå§‹åŒ–çš„æ–¹æ³•ã€‚å¦‚æœæˆ‘ä»¬è¿˜æ‰“ç®— transformer + text encoder ä¸€èµ·å¾®è°ƒï¼Œä¹Ÿå¯ä»¥ç»™ text encoder ï¼ˆè¿™é‡ŒæŒ‡çš„æ˜¯ clip è€Œä¸æ˜¯ T5ï¼‰æ³¨å…¥ã€‚

LoRA å±‚åˆå§‹åŒ–åä¸éœ€è¦å¦‚åŒä¸Šé¢çš„å…¶ä»–æ¨¡å—ä¸€èˆ¬å¯¹ç›¸å…³ `param.requires_grad` åšå¤„ç†ï¼Œé»˜è®¤æ˜¯ `True`ã€‚

```python
from mindone.diffusers._peft import LoraConfig

# now we will add new LoRA weights to the attention layers
transformer_lora_config = LoraConfig(
    r=args.lora_rank,
    lora_alpha=args.lora_rank,
    init_lora_weights="gaussian",
    target_modules=["to_k", "to_q", "to_v", "to_out.0"],
)
transformer.add_adapter(transformer_lora_config)

if args.train_text_encoder:
    text_lora_config = LoraConfig(
        r=args.lora_rank,
        lora_alpha=args.lora_rank,
        init_lora_weights="gaussian",
        target_modules=["q_proj", "k_proj", "v_proj", "out_proj"],
    )
    text_encoder_one.add_adapter(text_lora_config)

```


## æ¨¡å—ç²¾åº¦è®¾ç½®

ç²¾åº¦è®¾ç½®å¯¹æˆ‘ä»¬çš„å¾®è°ƒç»“æœæ¯”è¾ƒé‡è¦ï¼Œè¿™éƒ¨åˆ†ä»‹ç»ä¸€ä¸‹ flux lora å¾®è°ƒå®è·µæ—¶çš„å„ä¸ªæ¨¡å—çš„å‚æ•°ç²¾åº¦ã€è¿è¡Œç²¾åº¦çš„è®¾ç½®è¿‡ç¨‹ä»¥åŠåŸå› ã€‚å‡è®¾æˆ‘ä»¬åªå¾®è°ƒ transformers çš„éƒ¨åˆ†ã€‚é¦–å…ˆçœ‹ä¸€ä¸‹å‚æ•°é‡ 11.91Bï¼Œå…¶ä¸­å‚ä¸è®­ç»ƒçš„ LoRA å±‚å‚æ•°é‡å¤§çº¦ 123.36Mã€‚è€ƒè™‘åˆ°è®­ç»ƒæ€§èƒ½å’Œæ˜¾å­˜é—®é¢˜ï¼Œæˆ‘ä»¬æ²¡å¿…è¦æŠŠæ‰€æœ‰æ¨¡å—çš„å‚æ•°ç²¾åº¦éƒ½è®¾ç½®ä¸ºå…¨ç²¾åº¦ã€‚

```python
# æŸ¥çœ‹å‚æ•°é‡çš„æ ·ä¾‹ä»£ç 
all_params = sum(p.numel() for p in transformer.get_parameters())
trainable_params = sum(p.numel() for p in transformer.trainable_params())
```

vae, text_encoder and transformer å‚ä¸è®­ç»ƒçš„æ¨¡å—ï¼Œåªå‚ä¸å‰å‘è¿ç®—ï¼Œæƒé‡æ— éœ€ä¿æŒå…¨ç²¾åº¦ã€‚å¯¹äº LoRA å±‚ï¼Œæˆ‘ä»¬å¯ä»¥æš‚æ—¶ä½¿ç”¨ to_float() å®ç°æ‰‹åŠ¨æ··ç²¾ï¼Œä½¿ç”¨åŠç²¾åº¦è®¡ç®—ï¼Œä½†æ˜¯éœ€è¦ä¿è¯å…¶å‚æ•°çš„ç²¾åº¦æ˜¯å…¨ç²¾åº¦ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨ mindspore æ¡†æ¶çš„ä¼˜åŒ–å™¨ï¼Œæ¯”å¦‚ `nn.AdamWeightDecay`, å½“å‰æ˜¯æŒ‰ç…§å‚æ•°çš„ç²¾åº¦åšæ¢¯åº¦æ›´æ–°çš„ï¼Œè€Œä¸ä¼šåœ¨åå‘æ›´æ–°æƒé‡æ—¶è‡ªåŠ¨ upcastã€‚å‡å¦‚è®­ç»ƒå‚æ•°ä¹Ÿè®¾ç½®æˆåŠç²¾åº¦ï¼Œå˜æˆå®Œå…¨çš„åŠç²¾åº¦è®­ç»ƒï¼Œåœ¨æ¢¯åº¦æ›´æ–°æ—¶å¯èƒ½ä¼šå¯¼è‡´æº¢å‡ºï¼Œæ— æ³•æ­£å¸¸è®­ç»ƒã€‚

åç»­æ¡†æ¶å’Œæ¨¡å‹åº”è¯¥ä¼šæŒç»­ä¼˜åŒ–å‡ºæ›´æ˜“ç”¨çš„å†™æ³•å»å¯¹æ ‡ Accelerate æä¾›çš„å¯¹åº”æ··ç²¾è®­ç»ƒåŠŸèƒ½ã€‚

ä»¥ `args.mixed_precision = bf16` ä¸ºä¾‹ï¼Œå„æ¨¡å—çš„å‚æ•°ç²¾åº¦ã€è®¡ç®—ç²¾åº¦è®¾ç½®å¦‚ä¸‹ï¼š

| precision   | vae  | textencoders | transformers | LoRA layers |
| :---------: | :--: | :----------: | :----------: | :---------: |
| parameters  | bf16 | bf16         | bf16         | fp32        |
| computation | bf16 | bf16         | bf16         | bf16        |


ç›¸å…³ä»£ç ç‰‡æ®µä»¥ä¾›å‚è€ƒï¼š

```python
import mindspore as ms
from mindone.diffusers.training_utils import cast_training_params

# For mixed precision training we cast all non-trainable weights (vae, text_encoder and transformer) to half-precision
# as these weights are only used for inference, keeping weights in full precision is not required.
weight_dtype = ms.float32
if args.mixed_precision == "fp16":
    weight_dtype = ms.float16
elif args.mixed_precision == "bf16":
    weight_dtype = ms.bfloat16

vae.to(dtype=weight_dtype)
transformer.to(dtype=weight_dtype)
text_encoder_one.to(dtype=weight_dtype)
text_encoder_two.to(dtype=weight_dtype)

models = [transformer]
if args.train_text_encoder:
    models.extend([text_encoder_one])

# Make sure the trainable params are in float32.
if args.mixed_precision == "fp16" or args.mixed_precision == "bf16":
    # only upcast trainable parameters (LoRA) into fp32
    cast_training_params(models, dtype=ms.float32)

# Prepare everything with our `accelerator`.
# LoRA layer .to_float(weight_dtype)
for peft_model in models:
    for _, module in peft_model.cells_and_names():
        if isinstance(module, BaseTunerLayer):
            for layer_name in module.adapter_layer_names:
                module_dict = getattr(module, layer_name)
                for key, layer in module_dict.items():
                    if key in module.active_adapters and isinstance(layer, nn.Cell):
                        layer.to_float(weight_dtype)
```

æ³¨æ„åˆ°æˆ‘ä»¬åœ¨è½¬æ¢æ¨¡å‹ç²¾åº¦æ—¶ä½¿ç”¨äº† `.to(dtype)`ã€‚äº‹å®ä¸Š MindSpore Cell æš‚æ—¶æ²¡æœ‰ `.to(dtype)` æ¥å£ï¼Œè¿™æ˜¯ `mindONE.diffusers` åš MindSpore å…¼å®¹æ—¶ï¼Œé€šè¿‡æ¨¡å‹çš„åŸºæœ¬ç±» `ModelMixin` æ‰‹åŠ¨å®ç°çš„ `to` æ–¹æ³•ï¼Œä»¥ä¸‹ç¤ºä¾‹ä»£ç ä»¥ä¾›å‚è€ƒã€‚


```python
class ModelMixin(nn.Cell, ...):
    r"""
    Base class for all models.
    ...
    """
    ...
    def to(self, dtype: Optional[ms.Type] = None):
        for p in self.get_parameters():
            p.set_dtype(dtype)
        return self

# ALl models base on `ModelMixin`
class FluxTransformer2DModel(ModelMixin, ConfigMixin, PeftAdapterMixin, FromOriginalModelMixin):
    ...
```

## æ‰©å±•é˜…è¯»
- [MindSpore è‡ªåŠ¨æ··åˆç²¾åº¦](https://www.mindspore.cn/tutorials/zh-CN/master/beginner/mixed_precision.html)
- [flux æ¨¡å‹åŠ è½½ã€Controlnetåˆå§‹åŒ–ä¸æ¨¡å—ç²¾åº¦è®¾ç½®](flux_controlnet_load_models.md)

